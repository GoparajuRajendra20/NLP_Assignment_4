{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G_srM6x5Xzt6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Collection: Download Warren Buffett's annual letters to shareholders\n",
        "\n",
        "# Data Preprocessing\n",
        "with open(\"/content/sample_data/WarrenBuffet.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "_HfzDkHaregs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "\n",
        "# Define dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_text, block_size):\n",
        "        self.examples = []\n",
        "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
        "            self.examples.append(tokenized_text[i:i+block_size])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.examples[idx], dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQyCUy1rKCt",
        "outputId": "323524c8-db51-4e4e-af46-abf55c3e0440"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (81615 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "train_dataset = TextDataset(tokenized_text, block_size)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Load pre-trained GPT2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Fine-tune the model\n",
        "model.to(device)\n",
        "model.train()\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "Xfp4MtoDrPJ6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch in tqdm(train_loader):\n",
        "        inputs, labels = batch.to(device), batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL7YmqjerSR3",
        "outputId": "6f4aa971-5f1a-4399-9dc2-b3e024f78240"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [00:09<00:00, 16.66it/s]\n",
            "100%|██████████| 160/160 [00:09<00:00, 16.62it/s]\n",
            "100%|██████████| 160/160 [00:09<00:00, 16.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "eval_loss = 0\n",
        "eval_steps = 0\n",
        "eval_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "pd2Hz-tSrUqg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in tqdm(eval_loader):\n",
        "    inputs, labels = batch.to(device), batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        eval_loss += outputs.loss\n",
        "    eval_steps += 1\n",
        "\n",
        "eval_loss /= eval_steps\n",
        "perplexity = torch.exp(eval_loss)\n",
        "print(\"Perplexity:\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxkXJAeBrWkJ",
        "outputId": "a5758b0b-71fb-4987-b74c-2a5d0cc8bd55"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160/160 [00:02<00:00, 54.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: tensor(9.4722, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt_text, length=100, temperature=1.0):\n",
        "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, do_sample=True, max_length=length, temperature=temperature)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "h47sBdSurY5u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Warren Buffett believes\"\n",
        "generated_text = generate_text(model, tokenizer, prompt, length=200, temperature=0.7)\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SE_GQE2rakX",
        "outputId": "c230a4bd-7fd1-4c26-9b50-d1f33605998d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " Warren Buffett believes Berkshire's competitive advantage over the competition is important, and that we have to continue to use it. \n",
            "\n",
            "• We are a leading provider of supplemental insurance to all of our customers. If we were to lose some, we would need \n",
            "to take other insurers - one of whom would be a big loss - to replace it. We must also \n",
            "evaluate whether or not our current policies will adequately cover the cost of the cost-free premium we impose on \n",
            "our customers. \n",
            "\n",
            "• We get credit for the investment that we make. If we lose money on a Berkshire product, we pay the premiums to \n",
            "another company, who then adds us the money to pay for the risk we take on our customers. \n",
            "\n",
            "• When we qualify for certain types of supplemental insurance, we pay the premium for the \n",
            "contingent service, which is a far more expensive and expensive service than supplemental insurance. If we \n",
            "continue to operate under\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data Collection: Download Warren Buffett's annual letters to shareholders\n",
        "\n",
        "# Data Preprocessing\n",
        "with open(\"/content/sample_data/WarrenBuffet.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "R3b9pJrXp84M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Numerical Encoding\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "\n",
        "# Padding and Truncation\n",
        "max_length = 512  # Maximum length for GPT-2 models\n",
        "tokenized_text = tokenized_text[:max_length]  # Truncate to maximum length\n",
        "\n",
        "if len(tokenized_text) < max_length:\n",
        "    tokenized_text += [tokenizer.pad_token_id] * (max_length - len(tokenized_text))  # Pad if needed\n",
        "\n",
        "# Define dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_text, block_size):\n",
        "        self.examples = []\n",
        "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
        "            self.examples.append(tokenized_text[i:i+block_size])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.examples[idx], dtype=torch.long)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXCNMj6Zrnh3",
        "outputId": "5d151418-9bd9-45c1-91b6-b1196bab80a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (81615 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "train_dataset = TextDataset(tokenized_text, block_size)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Load pre-trained GPT2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Fine-tune the model\n",
        "model.to(device)\n",
        "model.train()\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "num_epochs = 3"
      ],
      "metadata": {
        "id": "cBLOIEx0rqwq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch in tqdm(train_loader):\n",
        "        inputs, labels = batch.to(device), batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBdJGPTPrtwI",
        "outputId": "5c86bb99-3e02-4c00-97b9-e7929563f122"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 23.29it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 21.00it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 18.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "eval_loss = 0\n",
        "eval_steps = 0\n",
        "eval_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "2ZUgxxPBrvbq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in tqdm(eval_loader):\n",
        "    inputs, labels = batch.to(device), batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        eval_loss += outputs.loss\n",
        "    eval_steps += 1\n",
        "\n",
        "eval_loss /= eval_steps\n",
        "perplexity = torch.exp(eval_loss)\n",
        "print(\"Perplexity:\", perplexity.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pnzypS8rxIH",
        "outputId": "4ed26347-8e2a-48b4-f4e4-5a054c5baa92"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 69.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 20.70240020751953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt_text, length=100, temperature=1.0):\n",
        "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id, do_sample=True, max_length=length, temperature=temperature)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "7fOWGrIyry9l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation\n",
        "prompt = \"Warren Buffett believes\"\n",
        "generated_text = generate_text(model, tokenizer, prompt, length=200, temperature=0.7)\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCjJj0-Qr0fx",
        "outputId": "604c8da1-4a62-4dcd-8eea-1932eb78fb66"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " Warren Buffett believes the market is a better bet than anyone else to see the U.S. economy recover in the next decade because of rising inequality.\n",
            "\n",
            "\"It's a tough year for the U.S. economy, because inequality has fallen in real terms and it's gotten worse over the last few years,\" Buffett said. \"There's more than enough capital in the U.S. economy to support a sustained recovery.\"\n",
            "\n",
            "He said that although the economy has grown faster than the U.S. economy since 2009, the number of workers has been less than half the amount they are today.\n",
            "\n",
            "Still, Buffett said that he is confident that \"there is momentum on the horizon.\"\n",
            "\n",
            "\"The U.S. economy is moving forward at a pace that we've never seen before,\" he said.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second generated text is more impressive due to several high-impact design choices:\n",
        "\n",
        "*   Relevance to Warren Buffett's Business Philosophy: The text discusses Berkshire Hathaway's competitive advantage, a topic that aligns closely with Warren Buffett's investment philosophy. Buffett often emphasizes the importance of identifying and capitalizing on a company's competitive strengths, making this discussion highly relevant.\n",
        "\n",
        "\n",
        "*   Clarity and Coherence: The text is clear and logically organized, presenting three key points about Berkshire's competitive advantage and how it contributes to the company's success. Each point is succinctly explained, maintaining coherence throughout. Use of Bullet Points: The use of bullet points enhances readability and makes the text more visually appealing. Bullet points allow for easy identification of key points and emphasize the importance of each aspect of Berkshire's competitive advantage.\n",
        "\n",
        "*   Incorporation of Specific Examples: The text provides specific examples to illustrate Berkshire's competitive advantage, such as its leading position in supplemental insurance and its ability to obtain credit for investments made. These examples add credibility and concreteness to the discussion.\n",
        "\n",
        "\n",
        "*   Engagement with Shareholders: The text addresses the importance of evaluating current policies and making strategic decisions to maintain Berkshire's competitive position. This engagement with shareholders reflects Buffett's communication style in his annual letters, where he often provides insights into the company's operations and strategy."
      ],
      "metadata": {
        "id": "uFmQdOwRtJ3s"
      }
    }
  ]
}